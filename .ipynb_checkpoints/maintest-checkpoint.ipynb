{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.distributions import Normal, Laplace, Independent, Bernoulli, Gamma, Uniform, Beta\n",
    "from torch.distributions.kl import kl_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, encoder_layers, decoder_layers, p_z, q_z, loss_dist):\n",
    "        super(VAE, self).__init__()\n",
    "        #self.distribution1 = dist1\n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "        self.p_z = p_z\n",
    "        self.q_z = q_z\n",
    "        self.loss_dist = loss_dist\n",
    "        \n",
    "    def encode(self, x):\n",
    "        out = self.encoder(x)\n",
    "        length_out = len(out[0]) // 2\n",
    "        return out[:,:length_out], out[:,length_out:]\n",
    "\n",
    "    def reparameterize(self, q_z_given_x):\n",
    "        return q_z_given_x.rsample()\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1,784)\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        \n",
    "        q_z_given_x = self.q_z(mu, logvar) # for KL divergence\n",
    "        q_z_given_x = Independent(q_z_given_x, 1)\n",
    "        \n",
    "        z = self.reparameterize(q_z_given_x)\n",
    "        x_hat = self.decode(z)\n",
    "\n",
    "        p_x_given_z = self.loss_dist(x_hat) # loss function/ distribution\n",
    "        p_x_given_z = Independent(p_x_given_z, 1)\n",
    "        \n",
    "        loss = self.loss_function(x_hat, x, q_z_given_x, p_x_given_z, z)\n",
    "        return x_hat, loss\n",
    "    \n",
    "    def loss_function(self, x_hat, x,q_z_given_x, p_x_given_z, z):\n",
    "        BCE = torch.sum(-p_x_given_z.log_prob(x))\n",
    "        #KLD = q_z_given_x.log_prob(z) - self.p_z.log_prob(z)\n",
    "        #print(KLD)\n",
    "        KLD = kl_divergence(q_z_given_x.base_dist, self.p_z.base_dist) # vervangen en werkend krijgen \n",
    "        KLD = torch.sum(KLD.sum(len(p_z.event_shape)-1))\n",
    "        return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_dist(mu, var):\n",
    "    return Normal(loc=mu, scale=var)\n",
    "\n",
    "def laplace_dist(mu, var):\n",
    "    return Laplace(loc=mu, scale=var)\n",
    "\n",
    "def gamma_dist(mu, var):\n",
    "    return Gamma(mu, var)\n",
    "\n",
    "def beta_dist(mu, var):\n",
    "    return Beta(mu, var)\n",
    "\n",
    "def bernoulli_loss(x_hat):\n",
    "    return Bernoulli(x_hat)\n",
    "\n",
    "def laplace_loss(x_hat):\n",
    "    return Laplace(loc=x_hat, scale=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data\n",
    "        optimizer.zero_grad()\n",
    "        x_hat, loss = model(data)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item() / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))\n",
    "\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            data = data\n",
    "            x_hat, loss = model(data)\n",
    "            test_loss += loss.item()\n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat([data[:n],\n",
    "                                      x_hat.view(128, 1, 28, 28)[:n]])\n",
    "                save_image(comparison.cpu(),\n",
    "                         'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(batch_size):\n",
    "    train_data = datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transforms.ToTensor())\n",
    "    train_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                               batch_size=batch_size, shuffle=True, **{})\n",
    "\n",
    "    test_data = datasets.MNIST('../data', train=False,\n",
    "                       transform=transforms.ToTensor())\n",
    "    test_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                           batch_size=batch_size, shuffle=True, **{})\n",
    "    return train_data, train_loader, test_data, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 543.125732\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 197.393829\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-7a331642183e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m#with torch.no_grad():\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-dcfca145e504>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
      "\u001b[0;32m~/anaconda/envs/medical_vae/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x_dim = 784\n",
    "z_dim = 2\n",
    "\n",
    "encoder_layers = [\n",
    "    nn.Linear(x_dim, 400),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(400, 40),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(40, z_dim*2),\n",
    "    nn.Softplus()\n",
    "    ]\n",
    "\n",
    "decoder_layers = [\n",
    "    nn.Linear(z_dim, 40),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(40, 400),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(400, x_dim),\n",
    "    nn.Sigmoid()\n",
    "    ]\n",
    "\n",
    "lr = 1e-3\n",
    "batch_size = 128\n",
    "epochs = 20\n",
    "\n",
    "# prior\n",
    "#p_z = Normal(loc=torch.zeros(1,z_dim), scale=1)\n",
    "p_z = Beta(torch.tensor([0.3, 0.3]), torch.tensor([0.3, 0.3]))\n",
    "p_z = Independent(p_z,1)\n",
    "# target distribution\n",
    "q_z = beta_dist\n",
    "\n",
    "# loss function\n",
    "loss_dist = bernoulli_loss\n",
    "\n",
    "train_data, train_loader, test_data, test_loader = load_data(batch_size)\n",
    "model = VAE(encoder_layers, decoder_layers, p_z, q_z, loss_dist)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "if __name__ == \"__main__\":\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(epoch)\n",
    "        test(epoch)\n",
    "        #with torch.no_grad():\n",
    "        #    sample = torch.randn(64, 20)\n",
    "        #    sample = model.decode(sample)\n",
    "        #    save_image(sample.view(64, 1, 28, 28),\n",
    "        #               'results/sample_' + str(epoch) + '.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "xv = np.arange(0, 4, .05)\n",
    "yv = np.arange(0, 4, .05)\n",
    "sample = np.zeros([len(yv)*len(xv), 2])\n",
    "counter = 0\n",
    "for i in xv:\n",
    "    for j in yv:\n",
    "        sample[counter] = [i, j]\n",
    "        counter += 1\n",
    "\n",
    "images = model.decode(torch.tensor(sample, dtype=torch.float)).detach().numpy()\n",
    "image = np.zeros([len(xv)*28, len(yv)*28])\n",
    "counter = 0\n",
    "for i in range(len(xv)):\n",
    "    for j in range(len(yv)):\n",
    "        image[i*28:i*28+28,j*28:j*28+28] = images[counter].reshape((28,28))\n",
    "        counter += 1\n",
    "        \n",
    "save_image(torch.tensor(image),'results/sample_norm2laplace' + str(epoch) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Linear(in_features=784, out_features=128, bias=True), ReLU(inplace), Linear(in_features=128, out_features=4, bias=True)]\n"
     ]
    }
   ],
   "source": [
    "x_dim = 784\n",
    "z_dim = 2\n",
    "encoder_layers = [\n",
    "    nn.Linear(x_dim, 128),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(128, z_dim*2)\n",
    "    ]\n",
    "\n",
    "decoder_layers = [\n",
    "    nn.Linear(2, 128),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(128, x_dim)\n",
    "    ]\n",
    "print(encoder_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
